{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import nltk\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Word2vec\n",
    "import gensim\n",
    "\n",
    "# Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, Activation, MaxPooling1D, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# DATASET\n",
    "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "TRAIN_SIZE = 0.8\n",
    "# TEXT CLENAING\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "# WORD2VEC \n",
    "W2V_SIZE = 300\n",
    "W2V_WINDOW = 7\n",
    "W2V_EPOCH = 32\n",
    "W2V_MIN_COUNT = 10\n",
    "# KERAS\n",
    "SEQUENCE_LENGTH = 300\n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 1024\n",
    "# SENTIMENT\n",
    "POSITIVE = \"POSITIVE\"\n",
    "NEGATIVE = \"NEGATIVE\"\n",
    "NEUTRAL = \"NEUTRAL\"\n",
    "SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
    "# EXPORT\n",
    "KERAS_MODEL = \"model.h5\"\n",
    "WORD2VEC_MODEL = \"model.w2v\"\n",
    "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
    "ENCODER_MODEL = \"encoder.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1600000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"../../../Datasets/sentiment140_tweet.csv\", encoding =DATASET_ENCODING , names=DATASET_COLUMNS)\n",
    "df.head()\n",
    "print(\"Dataset size:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def preprocess(text, stem=False):\n",
    "    # Remove link,user and special characters\n",
    "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df.text = df.text.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN size: 1280000\n",
      "TEST size: 320000\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=42)\n",
    "print(\"TRAIN size:\", len(df_train))\n",
    "print(\"TEST size:\", len(df_test))\n",
    "#documents = [_text.split() for _text in df_train.text] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "train_size = 30000\n",
    "test_size = 10000\n",
    "\n",
    "df = pd.DataFrame(None)\n",
    "\n",
    "texts_train = df_train[\"text\"][:train_size].tolist()\n",
    "ydata_train = df_train[\"target\"][:train_size].tolist()\n",
    "df_train = pd.DataFrame(None)\n",
    "\n",
    "texts_test = df_test[\"text\"][:test_size].tolist()\n",
    "ydata_test = df_test[\"target\"][:test_size].tolist()\n",
    "df_test = pd.DataFrame(None)\n",
    "\n",
    "\n",
    "local_model_file = \"../../models/inpsmt_e15_d200t200.bin\"\n",
    "external_hdd = \"/media/omer/Seagate Backup Plus Drive/OMER/FastText Models/\"\n",
    "metal_flash = \"/media/omer/UBUNTU 20_0/NOVA/models/\"\n",
    "\n",
    "model = fasttext.load_model(metal_flash + \"inpsmt_e15_d200t100.bin\")\n",
    "\n",
    "def one_hot_encode(alist):\n",
    "    ret = []\n",
    "    for value in alist:\n",
    "        vec = [0, 0, 0, 0, 0]\n",
    "        vec[value] = 1\n",
    "        ret.append(vec)\n",
    "    return ret\n",
    "\n",
    "# Turn text into vectors\n",
    "xdata_train, xdata_test = [], []\n",
    "for text in texts_train:\n",
    "    xdata_train.append(model.get_sentence_vector(text))\n",
    "    \n",
    "for text in texts_test:\n",
    "    xdata_test.append(model.get_sentence_vector(text))\n",
    "    \n",
    "x_train = np.array(xdata_train)\n",
    "y_train = np.array(one_hot_encode(ydata_train))\n",
    "\n",
    "x_test = np.array(xdata_test)\n",
    "y_test = np.array(one_hot_encode(ydata_test))\n",
    "\n",
    "del xdata_train[:]\n",
    "del xdata_test[:]\n",
    "del ydata_train[:]\n",
    "del ydata_test[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train,(-1,x_train.shape[1], 1))\n",
    "x_test = np.reshape(x_test,(-1, x_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(x_train[0])\n",
    "\n",
    "#create model\n",
    "model = Sequential()#add model layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, 3, activation='relu', input_shape=(dim, 1)))\n",
    "model.add(MaxPooling1D(2))\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(Conv1D(64, 3, activation='relu'))\n",
    "#model.add(MaxPooling1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "938/938 - 4s - loss: 0.6580 - accuracy: 0.6120\n",
      "Epoch 2/20\n",
      "938/938 - 4s - loss: 0.6189 - accuracy: 0.6533\n",
      "Epoch 3/20\n",
      "938/938 - 4s - loss: 0.6150 - accuracy: 0.6585\n",
      "Epoch 4/20\n",
      "938/938 - 4s - loss: 0.6097 - accuracy: 0.6616\n",
      "Epoch 5/20\n",
      "938/938 - 4s - loss: 0.6080 - accuracy: 0.6626\n",
      "Epoch 6/20\n",
      "938/938 - 4s - loss: 0.6039 - accuracy: 0.6665\n",
      "Epoch 7/20\n",
      "938/938 - 4s - loss: 0.5999 - accuracy: 0.6674\n",
      "Epoch 8/20\n",
      "938/938 - 4s - loss: 0.5965 - accuracy: 0.6711\n",
      "Epoch 9/20\n",
      "938/938 - 4s - loss: 0.5946 - accuracy: 0.6716\n",
      "Epoch 10/20\n",
      "938/938 - 4s - loss: 0.5904 - accuracy: 0.6757\n",
      "Epoch 11/20\n",
      "938/938 - 4s - loss: 0.5862 - accuracy: 0.6804\n",
      "Epoch 12/20\n",
      "938/938 - 4s - loss: 0.5826 - accuracy: 0.6803\n",
      "Epoch 13/20\n",
      "938/938 - 4s - loss: 0.5783 - accuracy: 0.6869\n",
      "Epoch 14/20\n",
      "938/938 - 4s - loss: 0.5745 - accuracy: 0.6901\n",
      "Epoch 15/20\n",
      "938/938 - 4s - loss: 0.5685 - accuracy: 0.6975\n",
      "Epoch 16/20\n",
      "938/938 - 4s - loss: 0.5648 - accuracy: 0.6985\n",
      "Epoch 17/20\n",
      "938/938 - 4s - loss: 0.5601 - accuracy: 0.7027\n",
      "Epoch 18/20\n",
      "938/938 - 4s - loss: 0.5551 - accuracy: 0.7080\n",
      "Epoch 19/20\n",
      "938/938 - 4s - loss: 0.5514 - accuracy: 0.7113\n",
      "Epoch 20/20\n",
      "938/938 - 4s - loss: 0.5459 - accuracy: 0.7150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff34e63fd60>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=20, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 4ms/step - loss: 0.6004 - accuracy: 0.6783\n",
      "test loss, test acc: [0.6004053354263306, 0.6783000230789185]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
