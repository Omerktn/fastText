{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import nltk\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Word2vec\n",
    "import gensim\n",
    "\n",
    "# Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, Activation, MaxPooling1D, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# DATASET\n",
    "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "TRAIN_SIZE = 0.8\n",
    "# TEXT CLENAING\n",
    "TEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "# WORD2VEC \n",
    "W2V_SIZE = 300\n",
    "W2V_WINDOW = 7\n",
    "W2V_EPOCH = 32\n",
    "W2V_MIN_COUNT = 10\n",
    "# KERAS\n",
    "SEQUENCE_LENGTH = 300\n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 1024\n",
    "# SENTIMENT\n",
    "POSITIVE = \"POSITIVE\"\n",
    "NEGATIVE = \"NEGATIVE\"\n",
    "NEUTRAL = \"NEUTRAL\"\n",
    "SENTIMENT_THRESHOLDS = (0.4, 0.7)\n",
    "# EXPORT\n",
    "KERAS_MODEL = \"model.h5\"\n",
    "WORD2VEC_MODEL = \"model.w2v\"\n",
    "TOKENIZER_MODEL = \"tokenizer.pkl\"\n",
    "ENCODER_MODEL = \"encoder.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1600000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"../../../Datasets/sentiment140_tweet.csv\", encoding =DATASET_ENCODING , names=DATASET_COLUMNS)\n",
    "df.head()\n",
    "print(\"Dataset size:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def preprocess(text, stem=False):\n",
    "    # Remove link,user and special characters\n",
    "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n",
    "    tokens = []\n",
    "    for token in text.split():\n",
    "        if token not in stop_words:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df.text = df.text.apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=42)\n",
    "print(\"TRAIN size:\", len(df_train))\n",
    "print(\"TEST size:\", len(df_test))\n",
    "#documents = [_text.split() for _text in df_train.text] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 30000\n",
    "test_size = 10000\n",
    "\n",
    "df = pd.DataFrame(None)\n",
    "\n",
    "texts_train = df_train[\"text\"][:train_size].tolist()\n",
    "ydata_train = df_train[\"target\"][:train_size].tolist()\n",
    "df_train = pd.DataFrame(None)\n",
    "\n",
    "texts_test = df_test[\"text\"][:test_size].tolist()\n",
    "ydata_test = df_test[\"target\"][:test_size].tolist()\n",
    "df_test = pd.DataFrame(None)\n",
    "\n",
    "\n",
    "local_model_file = \"../../models/inpsmt_e15_d200t200.bin\"\n",
    "external_hdd = \"/media/omer/Seagate Backup Plus Drive/OMER/FastText Models/\"\n",
    "metal_flash = \"/media/omer/UBUNTU 20_0/NOVA/models/\"\n",
    "\n",
    "model = fasttext.load_model(metal_flash + \"inpsmt_e15_d200t180.bin\")\n",
    "\n",
    "def one_hot_encode(alist):\n",
    "    ret = []\n",
    "    for value in alist:\n",
    "        vec = [0, 0, 0, 0, 0]\n",
    "        vec[value] = 1\n",
    "        ret.append(vec)\n",
    "    return ret\n",
    "\n",
    "# Turn text into vectors\n",
    "xdata_train, xdata_test = [], []\n",
    "for text in texts_train:\n",
    "    xdata_train.append(model.get_sentence_vector(text))\n",
    "    \n",
    "for text in texts_test:\n",
    "    xdata_test.append(model.get_sentence_vector(text))\n",
    "    \n",
    "x_train = np.array(xdata_train)\n",
    "y_train = np.array(one_hot_encode(ydata_train))\n",
    "\n",
    "x_test = np.array(xdata_test)\n",
    "y_test = np.array(one_hot_encode(ydata_test))\n",
    "\n",
    "del xdata_train[:]\n",
    "del xdata_test[:]\n",
    "del ydata_train[:]\n",
    "del ydata_test[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train,(-1,x_train.shape[1], 1))\n",
    "x_test = np.reshape(x_test,(-1, x_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = len(x_train[0])\n",
    "\n",
    "#create model\n",
    "model = Sequential()#add model layers\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(64, 3, activation='relu', input_shape=(dim, 1)))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Dropout(0.5))\n",
    "#model.add(Conv1D(64, 3, activation='relu'))\n",
    "#model.add(MaxPooling1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, epochs=20, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
